{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkchandalia/nlp/blob/part4/notebooks/1.0%20NLP_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB2JOABMiM4Q"
      },
      "source": [
        "# **Intro to Practical Hands-on Deep Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rht5jZf89DGB"
      },
      "source": [
        "# Structuring Unstructured Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgOqGyZ5hxed"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<p align=\"center\">\n",
        "<img src='https://drive.google.com/uc?export=view&id=1jhbAMekmaR8t8ZWpHN0Rf9y3rDKfzKr_' alt=\"History of LLMs\", width=\"900\" height=\"420\"/>\n",
        "<figcaption>Addition and Subtraction Operations on Different Data Types</figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qsTjkHTZRNbQ"
      },
      "source": [
        "#### *Comparing Sequence Data*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2zJKw3PM8-vL"
      },
      "source": [
        "#### Import Libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "L9vmG-hk894X"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#@title Import Libraries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#import pandas as pd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#import seaborn as sns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m#import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#%matplotlib inline\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
          ]
        }
      ],
      "source": [
        "#@title Import Libraries\n",
        "import numpy as np\n",
        "#import pandas as pd\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guMFeLT8Orhq",
        "outputId": "381677fd-507e-4273-cf2b-4135766a55a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is arr1 close to arr2:  True\n",
            "Is arr2 close to arr3:  False\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def array_isclose(arr1, arr2):\n",
        "  for i in range(len(arr1)):\n",
        "    if not(math.isclose(arr1[i], arr2[i], rel_tol = 0.1)):\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "arr1 = [2.3, 1.01, 4.5]\n",
        "arr2 = [2.2, 1.00, 4.3]\n",
        "arr3 = [1.8, 1.5, 4.5]\n",
        "\n",
        "print(\"Is arr1 close to arr2: \", array_isclose(arr1, arr2))\n",
        "print(\"Is arr2 close to arr3: \", array_isclose(arr2, arr3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54nPj9oICG1c"
      },
      "source": [
        "How about images? Or sentences? We will start by looking at two sentences that are similar:\n",
        "\n",
        "\n",
        "\n",
        "### 1.   **The cat took a nap on the rug.**\n",
        "<figure>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1KCjrHAU1V7P73OFjzNiabIR0nfKfpen7' alt=\"History of LLMs\", width=\"200\" height=\"200\"/>\n",
        "<figcaption>By DALL-E: Cat taking nap on rug</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "### 2.   **The feline slept in the sunshine.**\n",
        "<figure>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1gtHWnVm-fPSruzxjMS1GHK0bgLqYBYyo' alt=\"History of LLMs\", width=\"200\" height=\"200\"/>\n",
        "<figcaption>By DALL-E: Feline sleeping in sunshine</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "Do we feel like there’s a similar idea being presented in these two sentences? \n",
        "What about the below sentence?\n",
        "\n",
        "\n",
        "### 3.   **The cat took a bite of the rug.**\n",
        "\n",
        "<figure>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1DtnL-sFkhmFKnOAXPJ4q5n9z-eoZMIP0' alt=\"History of LLMs\", width=\"200\" height=\"200\"/>\n",
        "<figcaption>By DALL-E: Cat biting rug</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2xgdoyssU9G"
      },
      "source": [
        "#### Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfFtyiBJCtGg"
      },
      "source": [
        "Is there a way to quantify our feelings about the differences/similarities between these three sentences? Let's try something simple and represent each sentence as a bag of words:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjRSJvVYbZxp"
      },
      "source": [
        "Add a vocab step here to convert to numerical representation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvNGp-m_Cz0W"
      },
      "outputs": [],
      "source": [
        "s1 = {'The', 'cat', 'took', 'a', 'nap', 'on', 'the', 'rug'}\n",
        "s2 = {'The', 'feline', 'slept', 'in', 'the', 'sunshine'}\n",
        "s3 = {'The', 'cat', 'took', 'a', 'bite', 'of', 'the', 'rug'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6YOw3bIbZxq",
        "outputId": "c27b9ea0-2a12-4fab-9c1f-16b2506510e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between s1 and s2:  0.25\n",
            "Similarity between s1 and s3:  0.75\n",
            "Similarity between s2 and s3:  0.25\n"
          ]
        }
      ],
      "source": [
        "similarity_1_2 = len(s1.intersection(s2))/max(len(s1), len(s2))\n",
        "similarity_1_3 = len(s1.intersection(s3))/max(len(s1), len(s3))\n",
        "similarity_2_3 = len(s2.intersection(s3))/max(len(s2), len(s3))\n",
        "\n",
        "print(\"Similarity between s1 and s2: \", similarity_1_2)\n",
        "print(\"Similarity between s1 and s3: \", similarity_1_3)\n",
        "print(\"Similarity between s2 and s3: \", similarity_2_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I7Uow_XEawV"
      },
      "source": [
        "Using a bag of words approach, which sentences are more similar? Does this match our intuition for which sentences are most similar/dissimilar out of our examples?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMwGuiUDbZxr"
      },
      "source": [
        "Because ‘The’, ‘cat’, ‘took’, ‘a’, ‘the’, and ‘rug’ are common words in sentences 1 and 3, they appear to be more alike than sentences 1 and 2. \n",
        "\n",
        "What are the limitations of this approach?  How are we handling sentences of different length?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJE8zONoEhOM"
      },
      "source": [
        "To quantify differences between sentences, we need to represent them in a numerical way. However, we also need this numerical representation to reflect what the sentences actually mean, i.e., capture the semantic content of these sentences and words. How can we do that?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGUHKiJpEqyu"
      },
      "source": [
        "# Large Language Models (LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN8bYZtfHBo9"
      },
      "source": [
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1x0w2nrDUcuAUOqwbjpM8NKrH2T3m5gLH' alt=\"History of LLMs\", width=\"900\" height=\"600\"/>\n",
        "<figcaption>Recent History of Large Language Models (credit: ...)</figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CghvCIXbZxs"
      },
      "source": [
        "The explosion of advances in large language models comes at the confluence of large amounts of computing power, large amounts of data, training techniques that don't require explicit labels, and the transformer architecture with multiple attention layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iekX-FkXbZxs"
      },
      "source": [
        "## Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hey5iWMwF5LA"
      },
      "source": [
        "## Transformer Encoder\n",
        "\n",
        "To train a model like BERT that has 100M parameters, we need a lot of data. As some of you in the field may know, data is the fuel that powers our models and it can be hard to acquire and expensive to label. BERT is trained in a self-supervised way using masked language modeling and next sentence prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5CsPZV3cEyR"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<p align=\"center\">\n",
        "<img src='https://drive.google.com/uc?export=view&id=1-vIEv6GOo5EZ_XD9goAyVh52xUyWhGmo' alt=\"History of LLMs\", width=\"450\" height=\"300\"/>\n",
        "</p>\n",
        "<figcaption>Transformer Encoder (credit: ...)</figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFFkvoLQKXcU"
      },
      "source": [
        "### Self-supervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq75XULbDOog"
      },
      "source": [
        "*Masked Language Modelling (MLM)*\n",
        "\n",
        "Let’s look at the following sentence and try to predict what word should be in the sentence instead of the **< MASK >** placeholder. \n",
        "\n",
        "The clever **< MASK >** got the cheese without springing the trap. \n",
        "\n",
        "What do we think the word is? By training BERT to predict the correct word, but over hundreds of millions of sentences, we teach the model to learn relationships between words and become a better language model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP2eFWq9D_Nh"
      },
      "source": [
        "*Next Sentence Prediction (NSP)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5_ZmM4iwLeh"
      },
      "source": [
        "The other way BERT was trained was by taking two pairs of sentences and predicting if the two sentences are related, i.e., does sentence 2 follow logically after sentence 1. As an example, let’s take this first sentence:\n",
        "\n",
        "**The cat climbed up a tree and got stuck.**\n",
        "\n",
        "Now, let’s look at two possible next sentences: \n",
        "\n",
        "**1. Letters can be posted in person during business hours.**\n",
        "\n",
        "**2. The firefighter came with a ladder and climbed up to rescue the cat**\n",
        "\n",
        "Which one makes more sense? By also training BERT over the next sentence prediction, we also capture more semantic content in this model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ba7VbG5GSoW"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jQ1uAuDe4zG"
      },
      "source": [
        "![Image 1](https://drive.google.com/uc?export=view&id=1uQROXUt3W0Zzs7Z-7oLple3Vu_93NEXZ)\n",
        "![Image 2](https://drive.google.com/uc?export=view&id=1QSrCaM2tkfO92Jc5WpGC3yuZXRn-Sr2U)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0WOhKR4KHGX"
      },
      "source": [
        "Traditionally, modeling sequences has been challenging but the general idea is that we want to express the information contained in the sequence into a compressed form and use that compressed form to generate an output. The output could be a translation of the original sequence into a new language or a classification. Taking the example from above, this means the above sentences that are semantically similar will be mapped to something similar in the compressed form while dissimilar sentences will be further apart. What transformed (pun intended) this compression step is the concept of attention. \n",
        "\n",
        "I’m going to need to gloss over the technical details, but basically for each item in our sequence, we ask or query each item in our sequence to see how important or relevant it is for us. Of course this is done through matrix multiplications which we will not get into here. For this example sentence:\n",
        "\n",
        "“The cat purred in happiness”\n",
        "\n",
        "one could imagine that cat and purr attend to each other, likely because the english language reflects that cats purring is much more likely than say an elephant purring. After going through an attention layer, each item in the transformed sequence is actually a mixture of itself and all other items that contribute to the meaning of itself. By passing inputs through many such layers, we generate a representation of our original input that does capture the semantic meaning of our original input. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rSXb_KXGZOA"
      },
      "source": [
        "### Transfer Learning"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
