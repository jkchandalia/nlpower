{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning (<-Part 1):\n",
    "- What is fine-tuning? \n",
    "- Why fine-tune vs starting from scratch? \n",
    "\n",
    "# Section 2: Hands on / How to Apply Fine-Tuning\n",
    "\n",
    "Project Scoping:\n",
    "\n",
    "1. Present our business objective (increase adoption, improve customer satisfaction)  \n",
    "2. Develop a hypothesis ML solution to meet our objective (Sentiment classification on yelp reviews)\n",
    "3. Define success criteria of our experiment and scope of MVP model \n",
    "\n",
    "Conclusion: \n",
    "\n",
    "We've been given a business objective and ideal key results (OKR), we brainstormed hypotheses as to how we could leverage ML to meet our business objective. We selected a hypothesis to focus on. We narrowed the scope to an MVP experiment to evaluate if our app idea is actually useful and we've committed proactively to how we will track Key Performance Indicators to evaluate and verify if our app idea is helping us obtain the key results of our objective. \n",
    "\n",
    "Architecture scoping:\n",
    "1. Evaluate if fine-tuning is appropriate for our use-case  \n",
    "2. Narrow the scope to a category of base models \n",
    "   - hint we will touch more on high level landscape in part 4 (base model categories: NLP sequence modeling, generative vs vision, etc. )\n",
    "3. Choose a base model to assess baseline performance  \n",
    "   - high level landscape of sequence modeling base models to see where Bert is on the timeline & categorically\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "We've decided on a technical design approach to pursue: we've evaluated if fine-tuning was appropriate, we evaluated which base models were appropriate for our use-case and we chose a base model for our initial MVP to obtain baseline results quickly. (This helps us get immediate feedback as to if our project technical design seems feasible)\n",
    "\n",
    "#### Bert? (High Level)\n",
    "\n",
    "#### DistilBert - Why are we fine-tuning this model instead?\n",
    "\n",
    "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "It will be faster for us to train our model on DistilBERT. Best practice to create a model quickly to establish a baseline performance and iteratively add complexity if needed.  \n",
    "\n",
    "#### Intro to Hugging Face\n",
    "\n",
    "#### Fine tuning steps\n",
    "- create a dataset (convert from pandas to a hugging face dataset)\n",
    "- tokenize your training data with the same tokenizer used by the base model you are fine-tuning\n",
    "- \n",
    "\n",
    "#### Alternative fine-tuning methods (high level and resources for further learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
