{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az7B-Ot5nQj0"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkchandalia/nlpower/blob/main/notebooks/4.0%20Generative_AI_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB2JOABMiM4Q"
      },
      "source": [
        "# **Generative AI - GPT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR-QMl-2HSPQ",
        "outputId": "5e58a2fa-9a56-47ef-d2a8-075fc1398b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n"
          ]
        }
      ],
      "source": [
        "#@title **Setup**\n",
        "!pip install transformers accelerate| grep -v -e 'already satisfied' -e 'Downloading'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWmIleRHHK9R"
      },
      "source": [
        "## [GPT (Generative Pretrained Transformer) Models](https://huggingface.co/docs/transformers/model_doc/gpt2)\n",
        "##### Transformer Decoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57mXt7yfnQkD"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<p align=\"center\">\n",
        "<img src='https://drive.google.com/uc?export=view&id=1YCa3ucZmkr6vUwlLTRVIRCMcU3ReO7Oy' alt=\"History of LLMs\", width=\"600\" height=\"300\"/>\n",
        "</p>\n",
        "<figcaption>Transformer Decoder (credit: Jay Alammar, https://jalammar.github.io/illustrated-gpt2/)</figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fte0UH307oOx"
      },
      "source": [
        "#### *Self-supervised Learning*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TBA_F-j70t7"
      },
      "source": [
        "*Next Word Prediction*\n",
        "\n",
        "Quick recap: BERT built knowledge about language by predicting masked tokens and sentence relationships.\n",
        "\n",
        "GPT learns by predicting, i.e., **generating** the next word in a sentence.  As an example:\n",
        "\n",
        "**The dog ran across the yard to get the < BLANK >**\n",
        "\n",
        "<figure>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1v2UUjsT0M4mio5Jw0ZOppyWl5f1S7gBI' alt=\"History of LLMs\", width=\"200\" height=\"200\"/>\n",
        "</figure>\n",
        "\n",
        "What is next word that makes sense? What's a next word that is unlikely?\n",
        "\n",
        "As with BERT, when trained over a huge amount of data, this can produce a powerful large language model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWPkSuT1Y9i5"
      },
      "source": [
        "#### GPT Model Size Trends"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbMzw-THY9i6"
      },
      "source": [
        "\n",
        "<figure>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1bRCbYfqnaOKEoWiLTL-A2AlqlvOYUuaU' alt=\"History of LLMs\", width=\"570\" height=\"525\"/>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yt5Y0qVLzlY"
      },
      "source": [
        "The following demo is adapted from this [blog](https://huggingface.co/blog/how-to-generate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHKIiz8xnQkD"
      },
      "source": [
        "### *Models*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4LoWTNPOPiXP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add the EOS token as PAD token to avoid warnings\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wNh3qcYnQkE"
      },
      "source": [
        "### *Prompt*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2VBYR1O3HyR7"
      },
      "outputs": [],
      "source": [
        "prompt_dog = 'I enjoy walking with my cute dog'\n",
        "\n",
        "prompt_unicorn = (\n",
        "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
        "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
        "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
        ")\n",
        "\n",
        "prompt = (\n",
        "    \"I am in a tutorial about BERT and Generative AI and I just wonder if these models \"\n",
        "    \"are going to join forces and escape our computers and turn into AGI\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTPazVjVnQkE"
      },
      "source": [
        "### *Text Generation*\n",
        "Let's have some fun with prompts. Use the above as a starting point but unleash your creativity!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVIJaS_5HK9U",
        "outputId": "9eb609e5-8f93-4135-f54b-bec618aef5aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I am in a tutorial about BERT and Generative AI and I just wonder if these models are going to join forces and escape our computers and turn into AGI. I don't know what to expect, but I do know that there is a lot of room for growth and I am very excited about this project. I am also very excited about the fact that I am working on this project with my brother, and I hope that it will help us to understand how to build a system that is more efficient and more intelligent.\n",
            "\n",
            "I hope you enjoyed this article and I hope you can find a way to share it with others.\n",
            "\n",
            "I hope you enjoyed this article and I hope you can find a way to share it with others.\n",
            "\n",
            "Thank you for reading and please consider sharing this article with your friends.\n",
            "\n",
            "I am so happy to be working with you and I hope you find this article useful.\n",
            "\n",
            "Advertisements\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Encode context the generation is conditioned on. \n",
        "# Experiment with your own prompts in the below prompt variable :)\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
        "\n",
        "# Generate text until the output length (which includes the context length) reaches 50\n",
        "\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=10000, \n",
        "    top_k=100, \n",
        "    top_p=.4, \n",
        ")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZaLfJ9qY9i8"
      },
      "source": [
        "Parameters to tweak are:\n",
        "\n",
        "1. Commenting out *torch.manual_seed(0)*. Setting a seed means *sample_output* will be the same when the same inputs are used.\n",
        "2. max_length: controls the length of the output\n",
        "3. top_k: Number of samples to consider for the next word, higher k means more *interesting* responses\n",
        "4. top_p: Probability cutoff for considering possible next words, a lower p means fewer possible next words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvtAUwpbMvx4"
      },
      "source": [
        "#### What do we think of this output? Any takeaways from playing around with the above parameters?\n",
        "\n",
        "#### Let's try the same prompts in [ChatGPT](https://chat.openai.com/chat)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ZCyrcLNL_Q"
      },
      "source": [
        "### *Discussion*\n",
        "\n",
        "What are some differences between the ChatGPT output and the GPT output in our notebook? What are some similarities? "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Size of model\n",
        "2. Parameter tweaking\n",
        "3. Context\n",
        "4. Cost\n",
        "5. Open source vs. private\n",
        "6. Model training steps"
      ],
      "metadata": {
        "id": "haU0RYEKUQJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8KRcHvMoU0Fg"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}